{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import arviz as az\n",
    "import numpyro\n",
    "\n",
    "from hbmep.config import Config\n",
    "from hbmep.model.utils import Site as site\n",
    "\n",
    "PLATFORM = \"cpu\"\n",
    "jax.config.update(\"jax_platforms\", PLATFORM)\n",
    "numpyro.set_platform(PLATFORM)\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count() - 2\n",
    "numpyro.set_host_device_count(cpu_count)\n",
    "numpyro.enable_x64()\n",
    "numpyro.enable_validation()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro.distributions as dist\n",
    "from hbmep.model import BaseModel\n",
    "\n",
    "\n",
    "class MixtureModel(BaseModel):\n",
    "    NAME = \"mixture_model\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super(MixtureModel, self).__init__(config=config)\n",
    "        self.combination_columns = [self.subject] + self.features\n",
    "\n",
    "    def fn(self, x, a, b, v, L, ell, H):\n",
    "        return (\n",
    "            L\n",
    "            + jnp.where(\n",
    "                jnp.less(x, a),\n",
    "                0.,\n",
    "                -ell + jnp.true_divide(\n",
    "                    H + ell,\n",
    "                    jnp.power(\n",
    "                        1\n",
    "                        + jnp.multiply(\n",
    "                            -1\n",
    "                            + jnp.power(\n",
    "                                jnp.true_divide(H + ell, ell),\n",
    "                                v\n",
    "                            ),\n",
    "                            jnp.exp(jnp.multiply(-b, x - a))\n",
    "                        ),\n",
    "                        jnp.true_divide(1, v)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _model(self, subject, features, intensity, response_obs=None):\n",
    "        subject, n_subject = subject\n",
    "        features, n_features = features\n",
    "        intensity, n_data = intensity\n",
    "\n",
    "        intensity = intensity.reshape(-1, 1)\n",
    "        intensity = np.tile(intensity, (1, self.n_response))\n",
    "\n",
    "        feature0 = features[0].reshape(-1,)\n",
    "        n_feature0 = n_features[0]\n",
    "\n",
    "        with numpyro.plate(site.n_response, self.n_response):\n",
    "            \"\"\" Global Priors \"\"\"\n",
    "            b_scale_global_scale = numpyro.sample(\"b_scale_global_scale\", dist.HalfNormal(5))\n",
    "            v_scale_global_scale = numpyro.sample(\"v_scale_global_scale\", dist.HalfNormal(5))\n",
    "\n",
    "            L_scale_global_scale = numpyro.sample(\"L_scale_global_scale\", dist.HalfNormal(.5))\n",
    "            ell_scale_global_scale = numpyro.sample(\"ell_scale_global_scale\", dist.HalfNormal(10))\n",
    "            H_scale_global_scale = numpyro.sample(\"H_scale_global_scale\", dist.HalfNormal(5))\n",
    "\n",
    "            g_1_scale_global_scale = numpyro.sample(\"g_1_scale_global_scale\", dist.HalfNormal(5))\n",
    "            g_2_scale_global_scale = numpyro.sample(\"g_2_scale_global_scale\", dist.HalfNormal(5))\n",
    "\n",
    "            with numpyro.plate(\"n_feature0\", n_feature0):\n",
    "                \"\"\" Hyper-priors \"\"\"\n",
    "                a_mean = numpyro.sample(\"a_mean\", dist.TruncatedNormal(50., 20., low=0))\n",
    "                a_scale = numpyro.sample(\"a_scale\", dist.HalfNormal(30.))\n",
    "\n",
    "                b_scale_raw = numpyro.sample(\"b_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                b_scale = numpyro.deterministic(\"b_scale\", jnp.multiply(b_scale_global_scale, b_scale_raw))\n",
    "\n",
    "                v_scale_raw = numpyro.sample(\"v_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                v_scale = numpyro.deterministic(\"v_scale\", jnp.multiply(v_scale_global_scale, v_scale_raw))\n",
    "\n",
    "                L_scale_raw = numpyro.sample(\"L_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                L_scale = numpyro.deterministic(\"L_scale\", jnp.multiply(L_scale_global_scale, L_scale_raw))\n",
    "\n",
    "                ell_scale_raw = numpyro.sample(\"ell_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                ell_scale = numpyro.deterministic(\"ell_scale\", jnp.multiply(ell_scale_global_scale, ell_scale_raw))\n",
    "\n",
    "                H_scale_raw = numpyro.sample(\"H_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                H_scale = numpyro.deterministic(\"H_scale\", jnp.multiply(H_scale_global_scale, H_scale_raw))\n",
    "\n",
    "                g_1_scale_raw = numpyro.sample(\"g_1_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                g_1_scale = numpyro.deterministic(\"g_1_scale\", jnp.multiply(g_1_scale_global_scale, g_1_scale_raw))\n",
    "\n",
    "                g_2_scale_raw = numpyro.sample(\"g_2_scale_raw\", dist.HalfNormal(scale=1))\n",
    "                g_2_scale = numpyro.deterministic(\"g_2_scale\", jnp.multiply(g_2_scale_global_scale, g_2_scale_raw))\n",
    "\n",
    "                with numpyro.plate(site.n_subject, n_subject):\n",
    "                    \"\"\" Priors \"\"\"\n",
    "                    a = numpyro.sample(\n",
    "                        \"a\", dist.TruncatedNormal(a_mean, a_scale, low=0)\n",
    "                    )\n",
    "\n",
    "                    b_raw = numpyro.sample(\"b_raw\", dist.HalfNormal(scale=1))\n",
    "                    b = numpyro.deterministic(site.b, jnp.multiply(b_scale, b_raw))\n",
    "\n",
    "                    v_raw = numpyro.sample(\"v_raw\", dist.HalfNormal(scale=1))\n",
    "                    v = numpyro.deterministic(site.v, jnp.multiply(v_scale, v_raw))\n",
    "\n",
    "                    L_raw = numpyro.sample(\"L_raw\", dist.HalfNormal(scale=1))\n",
    "                    L = numpyro.deterministic(site.L, jnp.multiply(L_scale, L_raw))\n",
    "\n",
    "                    ell_raw = numpyro.sample(\"ell_raw\", dist.HalfNormal(scale=1))\n",
    "                    ell = numpyro.deterministic(\"ell\", jnp.multiply(ell_scale, ell_raw))\n",
    "\n",
    "                    H_raw = numpyro.sample(\"H_raw\", dist.HalfNormal(scale=1))\n",
    "                    H = numpyro.deterministic(site.H, jnp.multiply(H_scale, H_raw))\n",
    "\n",
    "                    g_1_raw = numpyro.sample(\"g_1_raw\", dist.HalfCauchy(scale=1))\n",
    "                    g_1 = numpyro.deterministic(site.g_1, jnp.multiply(g_1_scale, g_1_raw))\n",
    "\n",
    "                    g_2_raw = numpyro.sample(\"g_2_raw\", dist.HalfCauchy(scale=1))\n",
    "                    g_2 = numpyro.deterministic(site.g_2, jnp.multiply(g_2_scale, g_2_raw))\n",
    "\n",
    "        if response_obs is not None:\n",
    "            \"\"\" Outlier Distribution \"\"\"\n",
    "            outlier_prob = numpyro.sample(\"outlier_prob\", dist.Uniform(0., .01))\n",
    "            outlier_scale = numpyro.sample(\"outlier_scale\", dist.HalfNormal(10))\n",
    "\n",
    "        with numpyro.plate(site.n_response, self.n_response):\n",
    "            with numpyro.plate(site.n_data, n_data):\n",
    "                \"\"\" Model \"\"\"\n",
    "                mu = numpyro.deterministic(\n",
    "                    site.mu,\n",
    "                    self.fn(\n",
    "                        x=intensity,\n",
    "                        a=a[subject, feature0],\n",
    "                        b=b[subject, feature0],\n",
    "                        v=v[subject, feature0],\n",
    "                        L=L[subject, feature0],\n",
    "                        ell=ell[subject, feature0],\n",
    "                        H=H[subject, feature0]\n",
    "                    )\n",
    "                )\n",
    "                beta = numpyro.deterministic(\n",
    "                    site.beta,\n",
    "                    g_1[subject, feature0] + jnp.true_divide(g_2[subject, feature0], mu)\n",
    "                )\n",
    "\n",
    "                if response_obs is not None:\n",
    "                    q = numpyro.deterministic(\"q\", outlier_prob * jnp.ones((n_data, self.n_response)))\n",
    "                    bg_scale = numpyro.deterministic(\"bg_scale\", outlier_scale * jnp.ones((n_data, self.n_response)))\n",
    "\n",
    "                    mixing_distribution = dist.Categorical(\n",
    "                        probs=jnp.stack([1 - q, q], axis=-1)\n",
    "                    )\n",
    "                    component_distributions=[\n",
    "                        dist.Gamma(concentration=jnp.multiply(mu, beta), rate=beta),\n",
    "                        dist.HalfNormal(scale=bg_scale)\n",
    "                    ]\n",
    "\n",
    "                    \"\"\" Mixture \"\"\"\n",
    "                    Mixture = dist.MixtureGeneral(\n",
    "                        mixing_distribution=mixing_distribution,\n",
    "                        component_distributions=component_distributions\n",
    "                    )\n",
    "\n",
    "                    \"\"\" Observation \"\"\"\n",
    "                    numpyro.sample(\n",
    "                        site.obs,\n",
    "                        Mixture,\n",
    "                        obs=response_obs\n",
    "                    )\n",
    "\n",
    "                if response_obs is None:\n",
    "                    \"\"\" Observation \"\"\"\n",
    "                    numpyro.sample(\n",
    "                        site.obs,\n",
    "                        dist.Gamma(concentration=jnp.multiply(mu, beta), rate=beta),\n",
    "                        obs=response_obs\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:06:51,429 - hbmep.model.baseline - INFO - Initialized mixture_model\n"
     ]
    }
   ],
   "source": [
    "toml_path = \"/home/vishu/repos/hbmep-paper/configs/paper/tms/config.toml\"\n",
    "config = Config(toml_path=toml_path)\n",
    "config.BUILD_DIR = os.path.join(config.BUILD_DIR, \"model-comparison\", \"testing\", \"mixture-model\")\n",
    "config.MCMC_PARAMS[\"num_warmup\"] = 4000\n",
    "config.MCMC_PARAMS[\"num_samples\"] = 1000\n",
    "\n",
    "model = MixtureModel(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:06:56,077 - hbmep.dataset.core - INFO - Artefacts will be stored here - /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model\n",
      "2023-12-01 12:06:56,078 - hbmep.dataset.core - INFO - Copied config to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model\n",
      "2023-12-01 12:06:56,079 - hbmep.dataset.core - WARNING - Total non-positive observations: 0\n",
      "2023-12-01 12:06:56,080 - hbmep.dataset.core - WARNING - Total missing observations: 0\n",
      "2023-12-01 12:06:56,081 - hbmep.dataset.core - INFO - Processing data ...\n",
      "2023-12-01 12:06:56,082 - hbmep.utils.utils - INFO - func:load took: 0.01 sec\n"
     ]
    }
   ],
   "source": [
    "src = \"/home/vishu/data/hbmep-processed/human/tms/proc_2023-11-28.csv\"\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "subset = [\"SCA01\", \"SCA02\", \"SCS01\"]\n",
    "ind = df[model.subject].isin(subset)\n",
    "df = df[ind].reset_index(drop=True).copy()\n",
    "\n",
    "df, encoder_dict = model.load(df=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:06:59,465 - hbmep.model.baseline - INFO - Running inference with mixture_model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607fd024fdfd436db14bb301f046c8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214f52c0c60944e9bf6a85634ce51ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342117cfa94e45c7a4e659b9679f0d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551e7088d78d4655a69f82b26e0244c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcmc, posterior_samples = model.run_inference(df=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:58:25,496 - hbmep.utils.utils - INFO - func:make_prediction_dataset took: 0.00 sec\n",
      "2023-11-30 10:58:26,959 - hbmep.utils.utils - INFO - func:predict took: 1.46 sec\n",
      "2023-11-30 10:58:26,960 - hbmep.model.baseline - INFO - Rendering recruitment curves ...\n",
      "2023-11-30 10:58:28,118 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/recruitment_curves.pdf\n",
      "2023-11-30 10:58:28,118 - hbmep.utils.utils - INFO - func:render_recruitment_curves took: 1.16 sec\n",
      "2023-11-30 10:58:28,118 - hbmep.model.baseline - INFO - Rendering posterior predictive checks ...\n",
      "2023-11-30 10:58:29,381 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/posterior_predictive_check.pdf\n",
      "2023-11-30 10:58:29,382 - hbmep.utils.utils - INFO - func:predictive_checks_renderer took: 1.26 sec\n",
      "2023-11-30 10:58:29,382 - hbmep.utils.utils - INFO - func:render_predictive_check took: 1.26 sec\n"
     ]
    }
   ],
   "source": [
    "prediction_df = model.make_prediction_dataset(df=df)\n",
    "posterior_predictive = model.predict(df=prediction_df, posterior_samples=posterior_samples)\n",
    "\n",
    "orderby = lambda x: (x[1], x[0])\n",
    "model.render_recruitment_curves(df=df, encoder_dict=encoder_dict, posterior_samples=posterior_samples, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)\n",
    "model.render_predictive_check(df=df, encoder_dict=encoder_dict, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:21:09,176 - hbmep.utils.utils - INFO - func:make_prediction_dataset took: 0.00 sec\n",
      "2023-12-01 12:21:13,073 - hbmep.utils.utils - INFO - func:predict took: 3.90 sec\n",
      "2023-12-01 12:21:13,075 - hbmep.model.baseline - INFO - Rendering recruitment curves ...\n",
      "2023-12-01 12:21:18,616 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/recruitment_curves.pdf\n",
      "2023-12-01 12:21:18,617 - hbmep.utils.utils - INFO - func:render_recruitment_curves took: 5.54 sec\n",
      "2023-12-01 12:21:18,618 - hbmep.model.baseline - INFO - Rendering posterior predictive checks ...\n",
      "2023-12-01 12:21:23,632 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/posterior_predictive_check.pdf\n",
      "2023-12-01 12:21:23,633 - hbmep.utils.utils - INFO - func:predictive_checks_renderer took: 5.02 sec\n",
      "2023-12-01 12:21:23,633 - hbmep.utils.utils - INFO - func:render_predictive_check took: 5.02 sec\n"
     ]
    }
   ],
   "source": [
    "_posterior_samples = posterior_samples.copy()\n",
    "_posterior_samples[\"outlier_prob\"] = _posterior_samples[\"outlier_prob\"] * 0\n",
    "\n",
    "prediction_df = model.make_prediction_dataset(df=df)\n",
    "posterior_predictive = model.predict(df=prediction_df, posterior_samples=_posterior_samples)\n",
    "\n",
    "orderby = lambda x: (x[1], x[0])\n",
    "model.render_recruitment_curves(df=df, encoder_dict=encoder_dict, posterior_samples=_posterior_samples, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)\n",
    "model.render_predictive_check(df=df, encoder_dict=encoder_dict, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (0, 1), (2, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderby = lambda x: (x[1], x[0])\n",
    "combinations = model._make_combinations(df=df, columns=model.combination_columns, orderby=orderby)\n",
    "combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations = model._make_combinations(df=df, columns=model.combination_columns)\n",
    "combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant', 'participant_condition']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.combination_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:24:40,836 - hbmep.model.baseline - INFO - Rendering recruitment curves ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:24:45,901 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/recruitment_curves.pdf\n",
      "2023-12-01 12:24:45,902 - hbmep.utils.utils - INFO - func:render_recruitment_curves took: 5.07 sec\n",
      "2023-12-01 12:24:45,902 - hbmep.model.baseline - INFO - Rendering posterior predictive checks ...\n",
      "2023-12-01 12:24:50,935 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/posterior_predictive_check.pdf\n",
      "2023-12-01 12:24:50,937 - hbmep.utils.utils - INFO - func:predictive_checks_renderer took: 5.03 sec\n",
      "2023-12-01 12:24:50,937 - hbmep.utils.utils - INFO - func:render_predictive_check took: 5.03 sec\n"
     ]
    }
   ],
   "source": [
    "orderby = lambda x: (x[1], -x[0])\n",
    "model.render_recruitment_curves(df=df, encoder_dict=encoder_dict, posterior_samples=_posterior_samples, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)\n",
    "model.render_predictive_check(df=df, encoder_dict=encoder_dict, prediction_df=prediction_df, posterior_predictive=posterior_predictive, orderby=orderby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:19:24,086 - hbmep.utils.utils - INFO - func:make_prediction_dataset took: 0.01 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:19:27,997 - hbmep.utils.utils - INFO - func:predict took: 3.91 sec\n",
      "2023-12-01 12:19:27,997 - hbmep.model.baseline - INFO - Rendering recruitment curves ...\n",
      "2023-12-01 12:19:32,341 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/recruitment_curves.pdf\n",
      "2023-12-01 12:19:32,342 - hbmep.utils.utils - INFO - func:render_recruitment_curves took: 4.34 sec\n",
      "2023-12-01 12:19:32,342 - hbmep.model.baseline - INFO - Rendering posterior predictive checks ...\n",
      "2023-12-01 12:19:36,764 - hbmep.model.baseline - INFO - Saved to /home/vishu/repos/hbmep-paper/reports/paper/tms/model-comparison/testing/mixture-model/posterior_predictive_check.pdf\n",
      "2023-12-01 12:19:36,765 - hbmep.utils.utils - INFO - func:predictive_checks_renderer took: 4.42 sec\n",
      "2023-12-01 12:19:36,765 - hbmep.utils.utils - INFO - func:render_predictive_check took: 4.42 sec\n"
     ]
    }
   ],
   "source": [
    "_posterior_samples = posterior_samples.copy()\n",
    "_posterior_samples[\"outlier_prob\"] = _posterior_samples[\"outlier_prob\"] * 0\n",
    "\n",
    "prediction_df = model.make_prediction_dataset(df=df)\n",
    "posterior_predictive = model.predict(df=prediction_df, posterior_samples=_posterior_samples)\n",
    "\n",
    "model.render_recruitment_curves(df=df, encoder_dict=encoder_dict, posterior_samples=_posterior_samples, prediction_df=prediction_df, posterior_predictive=posterior_predictive)\n",
    "model.render_predictive_check(df=df, encoder_dict=encoder_dict, prediction_df=prediction_df, posterior_predictive=posterior_predictive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:58:57,190 - __main__ - INFO - Evaluating model ...\n",
      "/home/vishu/repos/hbmep-paper/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  warnings.warn(\n",
      "2023-11-30 10:58:58,662 - __main__ - INFO - ELPD LOO (Log): 1520.44\n",
      "/home/vishu/repos/hbmep-paper/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
      "See http://arxiv.org/abs/1507.04544 for details\n",
      "  warnings.warn(\n",
      "2023-11-30 10:58:58,686 - __main__ - INFO - ELPD WAIC (Log): 1499.86\n"
     ]
    }
   ],
   "source": [
    "numpyro_data = az.from_numpyro(mcmc)\n",
    "\n",
    "\"\"\" Model evaluation \"\"\"\n",
    "logger.info(\"Evaluating model ...\")\n",
    "\n",
    "score = az.loo(numpyro_data, var_name=site.obs)\n",
    "logger.info(f\"ELPD LOO (Log): {score.elpd_loo:.2f}\")\n",
    "\n",
    "score = az.waic(numpyro_data, var_name=site.obs)\n",
    "logger.info(f\"ELPD WAIC (Log): {score.elpd_waic:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:55:07,526 - __main__ - INFO - Evaluating model ...\n",
      "/home/vishu/repos/hbmep-paper/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  warnings.warn(\n",
      "2023-11-30 10:55:09,042 - __main__ - INFO - ELPD LOO (Log): 1520.44\n",
      "/home/vishu/repos/hbmep-paper/.venv/lib/python3.11/site-packages/arviz/stats/stats.py:1645: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
      "See http://arxiv.org/abs/1507.04544 for details\n",
      "  warnings.warn(\n",
      "2023-11-30 10:55:09,067 - __main__ - INFO - ELPD WAIC (Log): 1499.86\n"
     ]
    }
   ],
   "source": [
    "numpyro_data = az.from_numpyro(mcmc)\n",
    "\n",
    "\"\"\" Model evaluation \"\"\"\n",
    "logger.info(\"Evaluating model ...\")\n",
    "\n",
    "score = az.loo(numpyro_data, var_name=site.obs)\n",
    "logger.info(f\"ELPD LOO (Log): {score.elpd_loo:.2f}\")\n",
    "\n",
    "score = az.waic(numpyro_data, var_name=site.obs)\n",
    "logger.info(f\"ELPD WAIC (Log): {score.elpd_waic:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
